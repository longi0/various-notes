\providecommand{\main}{../../}
\documentclass[\main/main.tex]{subfiles}
\begin{document}

\subsection{Cosa si intende per Apprendimento con Rinforzo?}
L'apprendimento per rinforzo √® una tecnica di apprendimento automatico che viene utilizzata per ideare sistemi capaci di apprendere ed adattarsi all'ambiente che li circonda grazie ad una "ricompensa", detta rinforzo, che consiste nella valutazione delle loro prestazioni. L'apprendimento avviene mediante l'interazione con l'ambiente ed √® funzione del raggiungimento di uno o pi√π obiettivi.

RL √® un metodo di apprendimento che prevede un agente avendo a disposizione una serie di azioni, incomincia a interagire con l'ambiente, inizialmente sconosciuto, per raggiungere l'obbiettivo cercando di massimizzare la ricompensa a lungo termine.
\subsection{Quali sono gli attori?}
I principale attori quando si parla di reinforcement learning sono:
\begin{description}
  \item [Agente] software che svolge servizi per conto di un altro programma, solitamente in modo automatico ed invisibile. Formulato un problema l'agente cerca una soluzione e la implementa. Valuta inoltre la soluzione implementata e ragiona se ha avuto successo o no, se e quanto √® adeguata e cerca di ottimizzare le prestazioni per creare agenti migliori. L'agente sente l'input, modifica lo stato e genera un'azione che massimizza la ricompensa a lungo termine.
  \item [Stato] variabile interna all'agente che contiene una rappresentazione interna dell'ambiente. I possibili stati sono un insieme finito.
  \item [Azione] le possibili azioni applicabili sull'ambiente. Sono un insieme finito contenuto nell'agente. Viene selezionata l'azione da effettuare in base alla policy.
  \item [Policy] descrive l'azione scelta dall'agente. Mapping tra stato (input dall'ambiente) e azione. Funzione di controllo. Le policy possono avere una componente stocastica.
  \item [Ambiente] descrive tutto quello su cui agisce la policy. √à tutto quanto quello che non √® modificabile direttamente dall'agente. Si pu√≤ rappresentare come una funzione che preso uno stato e una azione come input restituisce un altro stato come output, ma √® una funzione non conosciuta a priori. L'agente deve costruirsi una rappresentazione implicita dell'ambiente attraverso la value function e deve selezionare i comportamenti che ripetutamente risultano favorevoli a lungo termine.
  \item [Segnale di rinforzo] √® un'informazione qualitativa (a volte binaria: giusto/sbagliato, successo/fallimento), puntuale. Due tipi: istantaneo, cio√® azione per azione (condizionamento classico), o una tantum, (condizionamente operante) cio√® viene rinforzata una catena di azioni, un comportamento. Viene generato all'esterno dell'agente e per esso rappresenta un input.
  \item [Reward/Quality function] √® la ricompensa immediata. Associata all'azione intrapresa in un certo stato. Pu√≤ essere data al raggiungimento di un goal. √à uno scalare (pu√≤ essere associato allo stato e/o input e/o stato prossimo).
  \item [Value function] ricompensa a lungo termine. Somma dei reward: costi associati alle azioni scelte istante per istante pi√π costo associato allo stato finale. Orizzonte temporale ampio. Rinforzo secondario. Ricompensa attesa. Viene stimata all'interno dell'agente.
  \item [Goal] obiettivo che deve raggiungere l'agente. Si pu√≤ aggiungere che l'agente deve raggiungere l'obiettivo con una policy ottima.
\end{description}

\subsection{Cosa rappresenta la critica?}
L'ambiente o l'interazione con esso pu√≤ essere complessa e/o variabile, inoltre il rinforzo pu√≤ avvenire solo dopo una pi√π o meno lunga sequenza di azioni (delayed reward).
Questo genera difficolt√† nell'analisi e nella valutazione di ambiente e azioni, questi problemi prendono il nome di: temporal credit assignment e structural credit assignment.
L'apprendimento non √® pi√π determinato da esempi, ma dall'osservazione del proprio comportamento nell'ambiente.
√à fondamentale definire come vengono assegnati i reward, che possono essere molto direzionati nel tempo (temporal CA problem) o distribuiti dall'interno di una struttura complessa (structural CA problem).
Il problema consiste nell'identificare quali siano le azioni che hanno inferito maggiormente nel raggiungimento di uno stato di goal ad alto reward.
La critica √® la value function stimata, ovvero una componente che definisce per ogni stato un'altra value function, che si unisce a quella dell'agente e in questo modo si rinforza. √à una value function alternativa.

\subsection{Che tipo di architettura si pu√≤ ipotizzare nell'apprendimento con rinforzo?}
L'architettura dei sistemi di RL si basa sulla retroazione. L'agente si muove nell'ambiente utilizzando una Policy, l'ambiente fornisce un reward e fa cambiare lo stato dell'agente il quale utilizza i risultati ottenuti per aggiornare la propria Policy cos√¨ da raggiungere il goal.
Tramite dei sensori l'agente monitora continuamente l'ambiente (input), tramite degli attuatori compie azioni che modificano lo stato dell'ambiente.

\subsection{Condizionamento classico e condizionamento operante}
Nel condizionamento classico il rinforzo viene eseguito istante per istante o azione per azione e permette di ottenere un riscontro ad ogni azione eseguita o ad ogni variazione dell'ambiente o dello stato.
Nel condizionamento operante invece il rinforzo avviene "una-tantum", viene quindi valutata una catena di azioni, un comportamento nel suo insieme e non nella singola azione.

\subsection{Quale relazione c'√® con l'intelligenza?}
Se consideriamo l'intelligenza come una funzione attiva, che permetta di prendere decisioni e compiere azioni nell'interazione con l'ambiente, il Reinforcement Learning consente di far comportare un agente in modo intelligente.

\subsection{Come potreste illustrare: Exploration vs Exploitation?}
L'esplorazione (exploration) consiste nel provare varie azioni per scoprire nuove possibili azioni(promettenti). Esplorazione dello spazio delle azioni per scoprire quelle migliori.
Un agente che esplora solamente raramente trover√† una buona soluzione.
Exploitation consiste nello scegliere sempre la soluzione che garantisca il miglior reward tra quelle conosciute (le informazioni vengono raccolte tramite exploration).
Se un agente non esplora nuove soluzioni potrebbe essere surclassato da nuovi agenti pi√π dinamici. Inoltre l'ambiente potrebbe essere mutevole e una soluzione potrebbe non restare valida. Occorre quindi non interrompere del tutto l'esplorazione e usare un approccio statistico per valutare le bont√† delle azioni bilanciando Exploration ed exploitation.

\subsection{Cos'√® il problema del credit assignement? √à un problema che riguarda la dimensione temporale o spaziale del task?}
I sistemi time-extended single-agent hanno il problema di valutare il contributo di un'azione rispetto alle altre (temporal credit assignment).
Questo problema viene accentuato se le azioni rilevanti sono temporalmente molto distanti tra di loro perch√© √® come se fossero spalmate su azioni non rilevanti e questo pu√≤ portare a cattive valutazioni. Un algoritmo dovrebbe riuscire a valutare l'influenza di azioni precedenti rispetto all'azione finale.
Sistemi multi-agente hanno inoltre il problema di determinare il contributo di un particolare agente ad un compito comune (structural credit assignment).

\subsection{Cos'√® l'eligibility trace (traccia) e quale √® il suo ruolo?}
L'eligibility trace √® un buffer di memoria contenente tracce di eventi passati.
Quando viene calcolato un errore usando metodi basati su TD (Temporal Difference), l'eligibility trace suggerisce quali variabili aggiornare.
Amplia l'orizzonte temporale sul quale fare l'aggiornamento a pi√π di 1 passo definendo cos√¨ se uno stato √® eleggibile e "quanto", cio√® che percentuale di aggiornamento meriti.
L'eligibility trace √® la base per risolvere i problemi di credit assignment.
Gli eventi passati che vengono chiamati tracce e rappresentano stati visitati, azioni compiute, ecc. evaporano nel tempo, cio√® diventano sempre meno rilevanti fino a scomparire.

\subsection{Definire l'algoritmo di Q-learning, descrivendo le equazioni opportune. [2]}
L'obiettivo di Q-learning √® quello di permettere ad un sistema di apprendimento automatico di adattarsi all'ambiente migliorando la scelta delle azioni da eseguire, per giungere a questo obiettivo cerca di massimizzare rispetto all'azione successivo. La funzione Q appresa, approssima direttamente Q*, indipendentemente dalla policy (off-policy).
Q\left(s_t\ ,\ a_t\right)=Q\left(s_t,a_t\right)+\alpha[r_{t+1}\ +\max\below{a_{t+1}}{Q\left(s_{t+1},a_{t+1}\right)-Q\left(s_t,a_t\right)}\ ]

\subsection{Scrivere le equazioni dell'algoritmo Q-learning in cui si consideri anche la traccia. [2]}
Q\left(s_t\ ,\ a_t\right)=Q\left(s_t,a_t\right)+\alpha\left[r_{t+1}+\max\below{a_{t+1}}{Q\left(s_{t+1},a_{t+1}\right)-Q\left(s_t,a_t\right)}\right]\ast e_t\left(s,a\right)

\subsection{Cosa si intende per politica epsilon-greedy? Come entra nell'algoritmo di Q-learning? }
La politica ùúÄ-greedy permette di scegliere con probabilit√† ùúÄ un'azione diversa dall'azione migliore conoscuita, in modo da esplorare le nuove possibilit√†.

\subsection{Che differenza c'√® tra Q-learning e SARSA? [2]}
Le differenze risiedono nel modo in cui viene aggiornato il valore di Q ad ogni azione verificatasi, SARSA √® on-policy mentre Q-Learning √® off-policy. Ovvero Q-Learning aggiorna il valore in funzione della migliore possibile scelta futura, mentre SARSA aggiorna il valore in funzione della scelta che verr√† applicata dalla policy. Nella pratica Q-learning converge pi√π lentamente di SARSA, ma √® in grado di imparare pi√π rapidamente nel caso l'ambiente si modifichi in quanto SARSA continuerebbe a seguire la propria policy.

\subsection{Dato un problema a piacere si descriva uno degli algoritmi visti a lezione e mostrare due passaggi di addestramento}
\subsection{Quale criterio si sceglie per definire i Reward? A quali elementi sono associati? Allo stato? All'azione? Allo stato prossimo? Perch√©? [2]}
L'ambiente fornisce un'informazione qualitativa, ad esempio success or fail.
L'informazione disponibile si chiama segnale di rinforzo e non d√† alcuna informazione su come aggiornare il comportamento dell'agente (e.g. i pesi).
√â per questo che il reward deve essere definito dall'agente in base alla politica adottata, e quindi si necessita di sistemi intelligenti che riescano a valutare al meglio le azioni da intraprendere.
In base alla politica adottata il reward sar√† legato allo stato, all'azione, allo stato prossimo o a un mix di essi.

\subsection{Impostare un problema su griglia (apprendimento del percorso di un agente, con partenza ed arrivo prescelti + ostacoli). La griglia fornisce un reward, diverso da zero, in ogni transizione. [2]}
\subsubsection{Definire chiaramente il problema, farne un modello definendo le variabili e le funzioni che le legano. [2]}
\subsubsection{Scrivere un risultato possibile dei primi 2 passi di apprendimento del problema definito al punto precedente. [2]}

\end{document}
